import streamlit as st
import pandas as pd
import os
import datetime
import json
from utils import generate_ai_question

# Configuration de la page
st.set_page_config(page_title="HR Eval Pro - Syst√®me d'√âvaluation", layout="wide", page_icon="üöÄ")

# Fonctions de base
def load_data(file_path):
    if os.path.exists(file_path):
        return pd.read_csv(file_path)
    return pd.DataFrame()

def save_data(df, file_path):
    df.to_csv(file_path, index=False)

def load_test_templates():
    template_file = 'test_templates.json'
    if os.path.exists(template_file):
        with open(template_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_test_templates(templates):
    template_file = 'test_templates.json'
    with open(template_file, 'w', encoding='utf-8') as f:
        json.dump(templates, f, ensure_ascii=False, indent=2)

def get_default_question_entretien(index):
    """Retourne les questions par d√©faut pour l'entretien structur√©."""
    questions = [
        "Parlez-moi d'un objectif de recrutement difficile que vous avez eu. Quelle √©tait votre strat√©gie pour l'atteindre et quel a √©t√© le r√©sultat ?",
        "D√©crivez une situation o√π un manager op√©rationnel n'√©tait pas d'accord avec votre s√©lection. Comment avez-vous argument√© et g√©r√© la situation ?",
        "Imaginez : 3 postes urgents √† pourvoir pour 3 managers diff√©rents qui attendent un retour rapide. Comment organisez-vous votre semaine ?"
    ]
    return questions[index] if index < len(questions) else ""

def get_default_tache(index):
    """Retourne les t√¢ches par d√©faut pour l'√©chantillon de travail."""
    taches = [
        "R√©digez un court e-mail d'approche directe sur LinkedIn pour un candidat passif.",
        "Donnez la requ√™te bool√©enne pour trouver un 'D√©veloppeur Python' avec 'Django' mais sans 'PHP'."
    ]
    return taches[index] if index < len(taches) else ""

# Initialisation des variables de session
if 'current_test_template' not in st.session_state:
    st.session_state.current_test_template = None
if 'evaluation_step' not in st.session_state:
    st.session_state.evaluation_step = 0
if 'evaluation_data' not in st.session_state:
    st.session_state.evaluation_data = {}

# Titre principal
st.title("üë• Syst√®me d'√âvaluation des Candidats - HR Eval Pro")

# Navigation par onglets principaux
main_tabs = st.tabs([
    "üìÅ Gestion",
    "‚öôÔ∏è Configuration", 
    "üìù √âvaluation",
    "üìö Biblioth√®que",
    "üìä Dashboard"
])

# Onglet Gestion
with main_tabs[0]:
    st.header("üìÅ Gestion des Tests d'√âvaluation")

    col_left, col_right = st.columns([2, 2])

    # Bloc "Cr√©er un test"
    with col_left:
        st.subheader("üÜï Cr√©er un Nouveau Test")

        # Informations de base du test
        col1, col2 = st.columns(2)
        with col1:
            test_name = st.text_input("Nom du test", key="test_name")
        with col2:
            test_category = st.selectbox("Cat√©gorie", ["Technique", "Commercial", "Management", "Support", "Autre"], key="test_category")

        col3, col4 = st.columns(2)
        with col3:
            test_poste = st.text_input("Poste associ√©", key="test_poste")

        if st.button("üíæ Cr√©er le test", type="primary"):
            if test_name:
                templates = load_test_templates()
                templates[test_name] = {
                                   "nom": test_name,
                    "categorie": test_category,
                    "poste": test_poste,
                    "date_creation": datetime.date.today().strftime("%Y-%m-%d"),
                    "questions_entretien": [],
                    "questions_cognitif": [],
                    "taches_echantillon": []
                }
                save_test_templates(templates)
                st.success(f"‚úÖ Test '{test_name}' cr√©√© avec succ√®s !")
                st.session_state.current_test_template = test_name
            else:
                st.error("Veuillez saisir un nom pour le test.")

    # Bloc "Chercher un test"
    with col_right:
        st.subheader("üîç Chercher un Test Existants")

        templates = load_test_templates()

        if templates:
            # Recherche par nom
            search_term = st.text_input("Rechercher par nom", key="search_test")

            # Filtrage par cat√©gorie
            categories = ["Toutes"] + list(set([t.get("categorie", "Autre") for t in templates.values()]))
            filter_category = st.selectbox("Filtrer par cat√©gorie", categories, key="filter_category")

            # Affichage des tests filtr√©s
            filtered_tests = {}
            for name, template in templates.items():
                if search_term.lower() in name.lower():
                    if filter_category == "Toutes" or template.get("categorie", "Autre") == filter_category:
                        filtered_tests[name] = template

            if filtered_tests:
                st.subheader("Tests trouv√©s :")
                for name, template in filtered_tests.items():
                    with st.expander(f"üìã {name} - {template.get('categorie', 'Autre')}"):
                        # affiche uniquement les informations essentielles
                        st.write(f"**Poste :** {template.get('poste', 'N/A')}")
                        st.write(f"**Cr√©√© le :** {template.get('date_creation', 'N/A')}")

                        col_a, col_b = st.columns(2)
                        with col_a:
                            if st.button(f"‚úèÔ∏è Modifier", key=f"edit_{name}"):
                                st.session_state.current_test_template = name
                                st.rerun()
                        with col_b:
                            if st.button(f"üóëÔ∏è Supprimer", key=f"delete_{name}"):
                                del templates[name]
                                save_test_templates(templates)
                                st.success(f"Test '{name}' supprim√© !")
                                st.rerun()
            else:
                st.info("Aucun test trouv√© avec ces crit√®res.")
        else:
            st.info("Aucun test n'a encore √©t√© cr√©√©. Cr√©ez votre premier test dans la colonne de gauche.")

# Onglet Configuration
with main_tabs[1]:
    st.header("‚öôÔ∏è Configuration des Tests")

    # Chargement du test courant (soit via Gestion -> Modifier, soit cr√©er)
    templates = load_test_templates()
    if not templates:
        st.warning("Aucun test n'existe encore. Cr√©ez d'abord un test dans l'onglet Gestion.")
        template = None
    else:
        if st.session_state.get('current_test_template'):
            sel_name = st.session_state.current_test_template
            template = templates.get(sel_name)
            if not template:
                st.error(f"Le test '{sel_name}' n'existe plus. S√©lectionnez ou cr√©ez un autre test dans Gestion.")
                template = None
        else:
            st.info("Cr√©ez un test dans 'Gestion' ou utilisez la recherche puis '‚úèÔ∏è Modifier' pour charger un test ici.")
            template = None

    # Affiche les sous-onglets uniquement si un template est charg√©
    if template:
        config_tabs = st.tabs(["‚öôÔ∏è Entretien Structur√©", "‚öôÔ∏è Test Cognitif", "‚öôÔ∏è √âchantillon de Travail"])

        # --- Configuration Entretien Structur√© ---
        with config_tabs[0]:
            st.subheader("Configuration de l'Entretien Structur√© (40%)")

            # Nombre de questions
            nb_questions = st.number_input("Nombre de questions", min_value=1, max_value=10,
                                         value=len(template.get("questions_entretien", [])) or 3,
                                         key="nb_questions_config")

            questions_entretien = []
            for i in range(nb_questions):
                # layout: main column + small AI chat column on right
                c_main, c_ai = st.columns([4,1])
                with c_main:
                    with st.expander(f"Question {i+1}", expanded=(i==0)):
                        existing_question = template.get("questions_entretien", []) if template else []
                        question_data = existing_question[i] if i < len(existing_question) else {"texte": get_default_question_entretien(i), "poids": 3}

                        q_col, s_col = st.columns([4,1])
                        with q_col:
                            question_text = st.text_area(f"Texte de la question {i+1}", height=80,
                                                       value=st.session_state.get(f"q_entretien_config_{i}", question_data["texte"]),
                                                       key=f"q_entretien_config_{i}")
                        with s_col:
                            poids_question = st.slider(f"Poids", 1, 5,
                                                     value=st.session_state.get(f"poids_entretien_config_{i}", question_data["poids"]),
                                                     key=f"poids_entretien_config_{i}")
                        questions_entretien.append({"texte": st.session_state.get(f"q_entretien_config_{i}"), "poids": poids_question})

                        # AI prompt + generate button placed inside the expander (compact)
                        ai_col, ai_btn = st.columns([4,1])
                        with ai_col:
                            ai_prompt = st.text_input(f"Prompt IA {i+1}", key=f"ai_entretien_prompt_{i}", placeholder="Ex: question sourcing")
                        with ai_btn:
                            if st.button(f"üí° G√©n√©rer", key=f"gen_entretien_ai_{i}"):
                                if ai_prompt:
                                    try:
                                        resp = generate_ai_question(ai_prompt, concise=True)
                                        if 'Question:' in resp:
                                            q = resp.split('Question:')[-1].split('\n')[0].strip()
                                        else:
                                            q = resp.split('\n')[0].strip()
                                        # injecte directement dans la zone de texte
                                        st.session_state[f"q_entretien_config_{i}"] = q
                                        st.session_state[f'ai_generated_entretien_{i}'] = q
                                        st.success("Question g√©n√©r√©e et ins√©r√©e")
                                        st.rerun()
                                    except Exception as e:
                                        st.error(f"Erreur IA: {e}")

            if st.button("üíæ Sauvegarder Configuration Entretien", key="save_entretien_config"):
                template["questions_entretien"] = questions_entretien
                save_test_templates(templates)
                st.success("Configuration de l'entretien sauvegard√©e !")

        # --- Configuration Test Cognitif ---
        with config_tabs[1]:
            st.subheader("Configuration du Test Cognitif (20%)")

            # possibilit√© d'ajouter plusieurs questions cognitives
            nb_cog = st.number_input("Nombre de questions cognitives", min_value=1, max_value=10,
                                     value=len(template.get("questions_cognitif", [])) or 1,
                                     key="nb_cognitif_config")

            questions_cognitif = []
            for i in range(nb_cog):
                c_main, c_ai = st.columns([4,1])
                with c_main:
                    with st.expander(f"Question cognitive {i+1}", expanded=(i==0)):
                        existing = template.get("questions_cognitif", []) if template else []
                        qdata = existing[i] if i < len(existing) else {"consigne": "", "poids": 3}
                        q_col, s_col = st.columns([4,1])
                        with q_col:
                            consigne = st.text_area(f"Consigne {i+1}", height=80, value=st.session_state.get(f"q_cog_{i}", qdata.get("consigne","")), key=f"q_cog_{i}")
                        with s_col:
                            poids = st.slider(f"√âvaluation (1-5)", 1,5, value=st.session_state.get(f"poids_cog_{i}", qdata.get("poids",3)), key=f"poids_cog_{i}")
                        questions_cognitif.append({"consigne": st.session_state.get(f"q_cog_{i}"), "poids": poids})

                        # AI prompt + generate button inside the expander (match Entretien layout)
                        ai_col, ai_btn = st.columns([4,1])
                        with ai_col:
                            ai_prompt = st.text_input(f"Prompt IA Cog {i+1}", key=f"ai_cog_prompt_{i}", placeholder="Ex: g√©n√©rer un cas logique")
                        with ai_btn:
                            if st.button(f"üí° G√©n√©rer", key=f"gen_cog_ai_{i}"):
                                if ai_prompt:
                                    try:
                                        resp = generate_ai_question(ai_prompt, concise=True)
                                        if 'Question:' in resp:
                                            q = resp.split('Question:')[-1].split('\n')[0].strip()
                                        else:
                                            q = resp.split('\n')[0].strip()
                                        # ins√®re directement dans la case de la consigne
                                        st.session_state[f"q_cog_{i}"] = q
                                        st.session_state[f'ai_generated_cog_{i}'] = q
                                        st.success("Question cognitive g√©n√©r√©e et ins√©r√©e")
                                        st.rerun()
                                    except Exception as e:
                                        st.error(f"Erreur IA: {e}")

            if st.button("üíæ Sauvegarder Configuration Cognitif", key="save_cognitif_config"):
                template["questions_cognitif"] = questions_cognitif
                save_test_templates(templates)
                st.success("Configuration du test cognitif sauvegard√©e !")

        # --- Configuration √âchantillon de Travail ---
        with config_tabs[2]:
            st.subheader("Configuration de l'√âchantillon de Travail (40%)")

            # Nombre de t√¢ches
            nb_taches = st.number_input("Nombre de t√¢ches", min_value=1, max_value=5,
                                      value=len(template.get("taches_echantillon", [])) or 2,
                                      key="nb_taches_config")

            taches_echantillon = []
            for i in range(nb_taches):
                c_main, c_ai = st.columns([4,1])
                with c_main:
                    with st.expander(f"T√¢che {i+1}", expanded=(i==0)):
                        existing_tache = template.get("taches_echantillon", [])
                        tache_data = existing_tache[i] if i < len(existing_tache) else {"texte": get_default_tache(i), "poids": 3}

                        tache_text = st.text_area(f"Consigne de la t√¢che {i+1}", height=80,
                                                value=tache_data["texte"],
                                                key=f"tache_config_{i}")
                        poids_tache = st.slider(f"Poids (1-5) t√¢che {i+1}", 1, 5,
                                              value=tache_data["poids"],
                                              key=f"poids_tache_config_{i}")
                        taches_echantillon.append({"texte": tache_text, "poids": poids_tache})

                        # AI prompt + generate button inside the expander
                        ai_col, ai_btn = st.columns([4,1])
                        with ai_col:
                            ai_prompt = st.text_input(f"Prompt IA T√¢che {i+1}", key=f"ai_task_prompt_{i}", placeholder="Ex: t√¢che sourcing")
                        with ai_btn:
                            if st.button(f"üí° G√©n√©rer", key=f"gen_task_ai_{i}"):
                                if ai_prompt:
                                    try:
                                        resp = generate_ai_question(ai_prompt, concise=True)
                                        if 'Question:' in resp:
                                            q = resp.split('Question:')[-1].split('\n')[0].strip()
                                        else:
                                            q = resp.split('\n')[0].strip()
                                        st.session_state[f"tache_config_{i}"] = q
                                        st.session_state[f'ai_generated_task_{i}'] = q
                                        st.success("T√¢che g√©n√©r√©e et ins√©r√©e")
                                        st.rerun()
                                    except Exception as e:
                                        st.error(f"Erreur IA: {e}")

            if st.button("üíæ Sauvegarder Configuration √âchantillon", key="save_echantillon_config"):
                template["taches_echantillon"] = taches_echantillon
                save_test_templates(templates)
                st.success("Configuration de l'√©chantillon de travail sauvegard√©e !")
    else:
        st.warning("Aucun test n'existe encore. Cr√©ez d'abord un test dans l'onglet Gestion.")

# Onglet √âvaluation
with main_tabs[2]:
    st.header("üìù √âvaluation du Candidat")

    # S√©lection du test √† utiliser (on utilise le template charg√© depuis Gestion)
    template = load_test_templates().get(st.session_state.get('current_test_template')) if st.session_state.get('current_test_template') else None
    if template:
        # Informations du candidat
        st.subheader("Informations du Candidat")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            nom_prenom = st.text_input("Nom et Pr√©nom du candidat", key="nom_prenom_eval")
        with col2:
            poste_candidat = st.text_input("Poste candidat√©", key="poste_candidat_eval")
        with col3:
            affectation = st.text_input("Affectation", key="affectation_eval")
        with col4:
            date_entretien = st.date_input("Date de l'entretien", datetime.date.today(), key="date_entretien_eval")

        # separation line between candidate info and evaluation steps
        st.markdown("---")

        # √âtapes d'√©valuation
        evaluation_steps = ["1Ô∏è‚É£ Entretien Structur√©", "2Ô∏è‚É£ Test Cognitif", "3Ô∏è‚É£ √âchantillon de Travail", "4Ô∏è‚É£ Synth√®se & D√©cision"]

        # Navigation entre √©tapes (full width)
        col_prev, col_current, col_next = st.columns([1, 2, 1])
        with col_prev:
            if st.session_state.evaluation_step > 0:
                if st.button("‚¨ÖÔ∏è Pr√©c√©dent", key="prev_step"):
                    st.session_state.evaluation_step -= 1
                    st.rerun()

        with col_current:
            current_step_name = evaluation_steps[st.session_state.evaluation_step]
            st.markdown(f"### {current_step_name}")

        with col_next:
            if st.session_state.evaluation_step < len(evaluation_steps) - 1:
                if st.button("Suivant ‚û°Ô∏è", key="next_step"):
                    st.session_state.evaluation_step += 1
                    st.rerun()

        # Contenu selon l'√©tape
        if st.session_state.evaluation_step == 0:  # Entretien Structur√©
            st.subheader("Entretien Structur√©")
            questions_entretien = template.get("questions_entretien", [])

            if questions_entretien:
                for i, question in enumerate(questions_entretien):
                    st.markdown(f"**Question {i+1} :** {question['texte']}")
                    notes_q = st.text_area(f"Notes Question {i+1} :", height=100, key=f"notes_q_eval_{i}")
                    note_q = st.slider(f"Note Question {i+1}", 1, 5, 3, key=f"note_q_eval_{i}")
                    st.session_state.evaluation_data[f"entretien_q{i}"] = {"notes": notes_q, "note": note_q}
            else:
                st.warning("Aucune question d'entretien configur√©e pour ce test.")

        elif st.session_state.evaluation_step == 1:  # Test Cognitif
            st.subheader("Test Cognitif")
            questions_cognitif = template.get("questions_cognitif", [])
            if questions_cognitif:
                for i, q in enumerate(questions_cognitif):
                    st.markdown(f"**Question cognitive {i+1} :** {q.get('consigne','')}")
                    col_q, col_s = st.columns([4,1])
                    with col_q:
                        reponse = st.text_area(f"R√©ponse {i+1}", height=120, key=f"reponse_cog_{i}")
                    with col_s:
                        note = st.slider(f"√âval (1-5)", 1,5,3, key=f"note_cog_{i}")
                    st.session_state.evaluation_data[f"cognitif_q{i}"] = {"reponse": reponse, "note": note}
            else:
                st.warning("Aucune question cognitive configur√©e pour ce test.")

        elif st.session_state.evaluation_step == 2:  # √âchantillon de Travail
            st.subheader("√âchantillon de Travail")
            taches_echantillon = template.get("taches_echantillon", [])

            if taches_echantillon:
                for i, tache in enumerate(taches_echantillon):
                    st.markdown(f"**T√¢che {i+1} :** {tache['texte']}")
                    reponse_tache = st.text_area(f"R√©ponse du candidat √† la t√¢che {i+1} :", height=100, key=f"reponse_tache_eval_{i}")
                    note_tache = st.slider(f"Note T√¢che {i+1}", 1, 5, 3, key=f"note_tache_eval_{i}")
                    st.session_state.evaluation_data[f"echantillon_t{i}"] = {"reponse": reponse_tache, "note": note_tache}
            else:
                st.warning("Aucune t√¢che d'√©chantillon configur√©e pour ce test.")

        elif st.session_state.evaluation_step == 3:  # Synth√®se & D√©cision
            st.subheader("Synth√®se et D√©cision Finale")

            # Calcul des scores
            questions_entretien = template.get("questions_entretien", [])
            taches_echantillon = template.get("taches_echantillon", [])

            # --- Normalisation des poids (√©chelle 1-5 en pourcentage) ---
            def normalize_weights(items):
                # items is list of dicts with 'poids' keys
                poids = [it.get('poids', 3) for it in items]
                total = sum(poids) if sum(poids) > 0 else len(poids)
                return [p / total for p in poids]

            # Score entretien (moyenne pond√©r√©e, les poids sont normalis√©s)
            score_entretien = 0
            if questions_entretien:
                normalized = normalize_weights(questions_entretien)
                for i, question in enumerate(questions_entretien):
                    eval_data = st.session_state.evaluation_data.get(f"entretien_q{i}", {"note": 3})
                    score_entretien += eval_data["note"] * normalized[i]

            # Score cognitif (moyenne pond√©r√©e si plusieurs questions)
            score_cognitif = 0
            questions_cognitif = template.get("questions_cognitif", [])
            if questions_cognitif:
                normalized_c = normalize_weights(questions_cognitif)
                for i, _ in enumerate(questions_cognitif):
                    eval_data = st.session_state.evaluation_data.get(f"cognitif_q{i}", {"note": 3})
                    score_cognitif += eval_data["note"] * normalized_c[i]
            else:
                score_cognitif = 3

            # Score √©chantillon (moyenne pond√©r√©e)
            score_echantillon = 0
            if taches_echantillon:
                normalized_t = normalize_weights(taches_echantillon)
                for i, tache in enumerate(taches_echantillon):
                    eval_data = st.session_state.evaluation_data.get(f"echantillon_t{i}", {"note": 3})
                    score_echantillon += eval_data["note"] * normalized_t[i]

            # Score final pond√©r√©: entretien 40%, cognitif 20%, √©chantillon 40%
            score_final = (score_entretien * 0.4) + (score_cognitif * 0.2) + (score_echantillon * 0.4)

            st.subheader(f"Score Final Pond√©r√© : {score_final:.2f} / 5.0")
            st.progress(score_final / 5)

            # Avis du manager et du recruteur
            col_manager, col_recruteur = st.columns(2)
            with col_manager:
                st.subheader("üëî Avis du Manager")
                avis_manager = st.text_area("Commentaires du manager", height=100, key="avis_manager")
                decision_manager = st.selectbox("D√©cision du manager",
                                              ["", "√Ä recruter", "√Ä recruter (avec r√©serves)", "Ne pas recruter"],
                                              key="decision_manager")

            with col_recruteur:
                st.subheader("üéØ Avis du Recruteur")
                avis_recruteur = st.text_area("Commentaires du recruteur", height=100, key="avis_recruteur")
                decision_recruteur = st.selectbox("D√©cision du recruteur",
                                                ["", "√Ä recruter", "√Ä recruter (avec r√©serves)", "Ne pas recruter"],
                                                key="decision_recruteur")

            points_forts = st.text_area("Points forts observ√©s", height=100, key="points_forts_final")
            axes_amelioration = st.text_area("Axes d'am√©lioration potentiels", height=100, key="axes_amelioration_final")

            if st.button("üíæ Finaliser l'√©valuation", type="primary"):
                if nom_prenom and poste_candidat and decision_manager and decision_recruteur:
                    file_path = 'evaluations_candidats.csv'
                    df_existing = load_data(file_path)

                    new_data = {
                        'Date': [date_entretien.strftime("%Y-%m-%d")],
                        'Nom et Pr√©nom': [nom_prenom],
                        'Poste': [poste_candidat],
                        'Affectation': [affectation],
                        'Test Utilis√©': [st.session_state.get('current_test_template')],
                        'Score Final': [round(score_final, 2)],
                        'Score Entretien': [round(score_entretien, 2)],
                        'Score Cognitif': [round(score_cognitif, 2)],
                        'Score √âchantillon': [round(score_echantillon, 2)],
                        'D√©cision Manager': [decision_manager],
                        'D√©cision Recruteur': [decision_recruteur],
                        'Avis Manager': [avis_manager],
                        'Avis Recruteur': [avis_recruteur],
                        'Points Forts': [points_forts],
                        'Axes Am√©lioration': [axes_amelioration]
                    }
                    df_new = pd.DataFrame(new_data)
                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)

                    save_data(df_combined, file_path)
                    st.success(f"√âvaluation de {nom_prenom} finalis√©e et sauvegard√©e !")
                    st.balloons()

                    # Reset pour nouvelle √©valuation
                    st.session_state.evaluation_step = 0
                    st.session_state.evaluation_data = {}
                else:
                    st.error("Veuillez renseigner toutes les informations obligatoires et les d√©cisions.")
    else:
        st.warning("Aucun test n'existe encore. Cr√©ez d'abord un test dans l'onglet Gestion.")

# Onglet Biblioth√®que
with main_tabs[3]:
    st.header("üìö Biblioth√®que des Tests - Historique")
    st.markdown("Consultez l'historique de toutes les √©valuations de candidats enregistr√©es localement.")

    file_path = 'evaluations_candidats.csv'
    df = load_data(file_path)

    if df.empty:
        st.warning("Aucune donn√©e d'√©valuation n'a √©t√© trouv√©e. Veuillez d'abord √©valuer un candidat dans l'onglet √âvaluation.")
    else:
        # --- KPIs ---
        st.header("Indicateurs Cl√©s")
        col1, col2, col3 = st.columns(3)
        col1.metric("Nombre de Candidats √âvalu√©s", len(df))
        col2.metric("Score Final Moyen", f"{df['Score Final'].mean():.2f} / 5")
        
        # Pourcentage de d√©cisions "√Ä recruter"
        try:
            decision_counts = df['D√©cision Manager'].value_counts(normalize=True)
            pct_a_recruter = decision_counts.get('√Ä recruter', 0) * 100
        except KeyError:
            pct_a_recruter = 0
        col3.metric("Taux d'embauche", f"{pct_a_recruter:.1f}%")

        st.markdown("---")
        
        # --- Visualisations ---
        st.header("Analyse des Scores")
        
        # Graphique de comparaison des scores
        entretien_moy = df['Score Entretien'].mean()
        cognitif_moy = df['Score Cognitif'].mean()
        echantillon_moy = df['Score √âchantillon'].mean()

        avg_scores_df = pd.DataFrame({
            'Cat√©gorie': ['Score Entretien', 'Score Cognitif', 'Score √âchantillon'],
            'Score Moyen': [entretien_moy, cognitif_moy, echantillon_moy]
        })
        st.bar_chart(avg_scores_df.set_index('Cat√©gorie'))
        st.caption("Comparaison des scores moyens par cat√©gorie d'√©valuation.")

        st.markdown("---")

        # --- Historique des Donn√©es ---
        st.header("Historique des √âvaluations")
        st.dataframe(df, use_container_width=True)
        st.caption("Vous pouvez trier et explorer les donn√©es en cliquant sur les en-t√™tes de colonnes.")

        # Option d'export
        if st.button("üì• Exporter les donn√©es (CSV)"):
            csv_data = df.to_csv(index=False)
            st.download_button(
                label="T√©l√©charger le fichier CSV",
                data=csv_data,
                file_name="historique_evaluations_candidats.csv",
                mime="text/csv"
            )

# Onglet Dashboard
with main_tabs[4]:
    st.header("üìä Dashboard & Statistiques Avanc√©es")

    file_path = 'evaluations_candidats.csv'
    df = load_data(file_path)

    if df.empty:
        st.warning("Aucune donn√©e d'√©valuation n'a √©t√© trouv√©e.")
    else:
        # KPIs principaux
        st.subheader("Indicateurs Cl√©s de Performance")

        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Nombre de Candidats √âvalu√©s", len(df))
        col2.metric("Score Final Moyen", f"{df['Score Final'].mean():.2f} / 5" if 'Score Final' in df.columns else "N/A")
        col3.metric("Taux d'Embauche (Manager)", f"{(df['D√©cision Manager'] == '√Ä recruter').mean() * 100:.1f}%" if 'D√©cision Manager' in df.columns else "N/A")
        col4.metric("Taux d'Embauche (Recruteur)", f"{(df['D√©cision Recruteur'] == '√Ä recruter').mean() * 100:.1f}%" if 'D√©cision Recruteur' in df.columns else "N/A")

        st.markdown("---")

        # Graphiques avanc√©s
        st.subheader("Analyse D√©taill√©e des Performances")

        if len(df) > 0:
            # Graphique des scores moyens
            col_a, col_b = st.columns(2)

            with col_a:
                st.subheader("Scores par Cat√©gorie")
                if all(col in df.columns for col in ['Score Entretien', 'Score Cognitif', 'Score √âchantillon']):
                    scores_df = pd.DataFrame({
                        'Cat√©gorie': ['Entretien', 'Cognitif', '√âchantillon'],
                        'Score Moyen': [
                            df['Score Entretien'].mean(),
                            df['Score Cognitif'].mean(),
                            df['Score √âchantillon'].mean()
                        ]
                    })
                    st.bar_chart(scores_df.set_index('Cat√©gorie'))

            with col_b:
                st.subheader("D√©cisions par √âvaluateur")
                if 'D√©cision Manager' in df.columns and 'D√©cision Recruteur' in df.columns:
                    decisions_df = pd.DataFrame({
                        'D√©cision': ['√Ä Recruter', 'R√©serves', 'Refuser'],
                        'Manager': [
                            (df['D√©cision Manager'] == '√Ä recruter').sum(),
                            (df['D√©cision Manager'] == '√Ä recruter (avec r√©serves)').sum(),
                            (df['D√©cision Manager'] == 'Ne pas recruter').sum()
                        ],
                        'Recruteur': [
                            (df['D√©cision Recruteur'] == '√Ä recruter').sum(),
                            (df['D√©cision Recruteur'] == '√Ä recruter (avec r√©serves)').sum(),
                            (df['D√©cision Recruteur'] == 'Ne pas recruter').sum()
                        ]
                    })
                    st.bar_chart(decisions_df.set_index('D√©cision'))

        # Statistiques d√©taill√©es
        st.subheader("Statistiques D√©taill√©es")
        if len(df) > 0:
            st.dataframe(df.describe(), use_container_width=True)

        # Analyse par test utilis√©
        if 'Test Utilis√©' in df.columns:
            st.subheader("Analyse par Test Utilis√©")
            test_stats = df.groupby('Test Utilis√©').agg({
                'Score Final': ['count', 'mean'],
                'Score Entretien': 'mean',
                'Score Cognitif': 'mean',
                'Score √âchantillon': 'mean'
            }).round(2)
            st.dataframe(test_stats, use_container_width=True)
